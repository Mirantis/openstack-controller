#apiVersion: lcm.mirantis.com/v1alpha1
#kind: HelmBundle

{%- set service = 'cinder' %}
{%- set components_with_dedicated_messaging = spec.get('features', {}).get('messaging', {}).get('components_with_dedicated_messaging', []) %}
{%- set stacklight_enabled = spec.get('features', {}).get('stacklight', {}).get('enabled', False) %}
{%- set notification_topics = ['notifications'] %}
{%- do notification_topics.append('stacklight_notifications') if stacklight_enabled %}
{%- set node_specific = {} %}
{%- set overrides = namespace(enabled=false) %}
{%- for label, node_features in spec.get("nodes", {}).items() %}
  {%- if "cinder" in node_features.get("features", {}).keys() %}
    {% set overrides.enabled = true %}
  {%- endif  %}
  {%- if node_features.get("features", {}) %}
    {%- do node_specific.update({label: node_features.features}) %}
  {%- endif %}
{%- endfor %}
{%- set glance_cinder_backends = spec.get('features', {}).get('glance', {}).get('backends', {}).get('cinder', {}) %}
{%- set glance_cinder_multi = {} %}
{%- set cinder_db_cleanup = spec.get('features', {}).get('database', {}).get('cleanup', {}).get('cinder', {'enabled': true}) %}

spec:
  releases:
{%- if 'block-storage' in components_with_dedicated_messaging %}
  - name: openstack-cinder-rabbitmq
    chart: {{spec.common.infra.repo}}/rabbitmq
    values:
{% include 'base/_rabbitmq_images.yaml' %}
  {%- if stacklight_enabled %}
      monitoring:
        prometheus:
          enabled: true
  {%- endif %}
      pod:
        replicas:
          server: 1
      manifests:
        network_policy: false
        job_users_create: true
      volume:
        enabled: false
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_messaging_dedicated.yaml' %}
      conf:
        users:
          cinder:
            auth:
              service_user:
                username: {{ credentials.messaging.user.username }}
                password: {{ credentials.messaging.user.password }}
            path: /cinder
        aux_conf:
          policies:
          - vhost: cinder
            name: default-policy
            pattern: '^(?!amq\.).*'
            definition:
              message-ttl: 120000
          - vhost: cinder
            name: results_expire
            pattern: '^results\.'
            definition:
              expires: 3600000
            priority: 1
          - vhost: cinder
            name: tasks_expire
            pattern: '^tasks\.'
            definition:
              expires: 3600000
            priority: 1
  {%- if stacklight_enabled %}
        prometheus_exporter:
          rabbit_exporters: "overview,exchange,node"
  {%- endif %}
{%- endif %}
{%- if overrides.enabled %}
  - name: openstack-iscsi
    chart: {{spec.common.infra.repo}}/iscsi
    values:
      images:
        tags:
{%- for image in [
    "dep_check",
    "iscsi_iscsi",
    "iscsi_tgt",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      manifests:
        daemonset_tgt: true
        daemonset_iscsi: false
{%- endif %}
{%- if spec.get('migration', {}).get('cinder', {}).get('deploy_main_service', True) %}
  - name: openstack-cinder
    chart: {{spec.common.openstack.repo}}/cinder
    values:
      images:
        tags:
{%- for image in [
    "db_drop",
    "image_repo_sync",
    "cinder_api",
    "cinder_scheduler",
    "db_init",
    "dep_check",
    "cinder_db_sync",
    "cinder_db_sync_online",
    "cinder_db_purge",
    "cinder_backup",
    "ks_user",
    "ks_service",
    "cinder_volume_usage_audit",
    "cinder_backup_storage_init",
    "ks_endpoints",
    "bootstrap",
    "cinder_storage_init",
    "rabbit_init",
    "cinder_service_cleaner",
    "cinder_volume",
    "cinder_volume_daemonset",
    "cinder_drop_default_volume_type",
    "cinder_create_internal_tenant",
    "test",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      dependencies:
        static:
          db_init:
            jobs:
              - openstack-mariadb-cluster-wait
      pod:
        replicas:
          api: 1
          registry: 1
      storage: ceph
      ceph_client:
        configmap: rook-ceph-config
        user_secret_name: {{ ceph.cinder.secrets }}
      conf:
        policy.d:
          01-controller-default.yaml: {{ service_policy }}
          02-custom.yaml: {{ spec.get("features", {}).get("policies", {}).get("cinder", {}) }}
        {%- if overrides.enabled %}
        standalone_backends:
          daemonset:
            conf:
              DEFAULT:
                cluster: ""
        {%- endif %}
        backends:
{%- set enabled_backends=[] %}
{%- for backend, backend_config in ceph.cinder.pools.items() %}
  {%- if backend_config.role == 'volumes' %}
    {%- do enabled_backends.append(backend) %}
          {{ backend }}:
    {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
      {%- if g_backend_opts.get('backend_name') %}
        {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
      {%- endif %}
      {%- if glance_cinder_volume_type == 'rbd' and glance_cinder_backend_name == backend %}
            image_upload_use_cinder_backend: True
            image_upload_use_internal_tenant: True
      {%- endif %}
    {%- endfor %}
            volume_driver: cinder.volume.drivers.rbd.RBDDriver
            volume_backend_name: {{ backend }}
            rbd_pool: {{ backend_config.name }}
            rbd_user: {{ ceph.cinder.username }}
            rbd_ceph_conf: "/etc/ceph/ceph.conf"
  {%- endif %}
{%- endfor %}
        ceph:
          pools:
            backup:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
            cinder.volumes:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
        cinder:
          keystone_authtoken:
            memcache_security_strategy: ENCRYPT
            memcache_secret_key: {{ credentials.memcached }}
          DEFAULT:
            # use unique host for both cluster and non-cluster mode,
            # otherwise messaging between API and cinder-volume will
            # be broken
            host: "<None>"
# NOTE(vsaienko): active/active mode is supported by rbd starting from Rocky
{%- if spec.openstack_version not in ['queens'] %}
            cluster: "cinder-ceph-cluster"
{%- endif %}
{%- if spec.get('features', {}).get('cinder', {}).get('backup', {}).get('backend') == 'ceph' %}
            backup_driver: cinder.backup.drivers.ceph.CephBackupDriver
{%- for backend, backend_config in ceph.cinder.pools.items() %}
  {%- if backend_config.role == 'backup' %}
            backup_ceph_user: {{ ceph.cinder.username }}
            backup_ceph_pool: {{ backend_config.name }}
  {%- endif %}
{%- endfor %}
{%- endif %}
            enabled_backends: {{ enabled_backends|join(',') }}
            default_volume_type: {{ enabled_backends[0] }}
            scheduler_default_filters: AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter,InstanceLocalityFilter
          oslo_messaging_notifications:
            topics: {{ notification_topics|join(',') }}
          coordination:
            backend_url: "etcd3+http://etcd:2379"
          service_user:
            send_service_user_token: true
          backend_defaults:
            # NOTE(vsaienko): Use dedicated pool for cinder only, this will allow to improve
            # scale characteristics
            rbd_exclusive_cinder_pool: true
          # Needed for InstanceLocalityFilter
          nova:
            auth_section: keystone_authtoken
            auth_type: password
{%- if spec.get('features', {}).get('glance', {}).get("signature", {}).get("enabled", False) %}
          glance:
            verify_glance_signatures: true
{%- endif %}
        logging:
          logger_cinder:
            level: {{ spec.get('features', {}).get('logging', {}).get('cinder', {}).get('level', 'INFO') }}
      # NOTE(vsaienko): do not create backends from .Values.conf.cinder.DEFAULT.backends
      # as we have default backend in chart values rbd1 which is not used.
      # Do not create volume type for backup, as user can't use this backend directly.
      bootstrap:
        bootstrap_conf_backends: false
        volume_types:
    {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
      {%- if g_backend_opts.get('backend_name') %}
        {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
          {{ glance_cinder_backend_name }}_multiattach:
            volume_backend_name: {{ glance_cinder_backend_name }}
            multiattach: "\"<is> True\""
      {%- endif %}
    {%- endfor %}
{%- for backend in enabled_backends %}
          {{ backend }}:
            volume_backend_name: {{ backend }}
{%- endfor %}
{%- for label, override in node_specific.items() %}
  {%- set cinder_override = override.get("cinder", {}) %}
    {%- if cinder_override %}
      {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
          {{ backend_name }}:
            volume_backend_name: {{ backend_name }}
      {%- endfor %}
    {%- endif %}
{%- endfor %}
      secrets:
        rbd:
          volume: {{ ceph.cinder.secrets }}
          backup: {{ ceph.cinder.secrets }}
      manifests:
        cron_job_db_purge: {{ cinder_db_cleanup.enabled }}
        job_drop_default_volume_type: true
        network_policy: false
        job_rabbit_init: false
        job_storage_init: false
        job_backup_storage_init: false
        secret_ca_bundle: true
        cron_service_cleaner: true
        cron_volume_usage_audit: false
      # NODE SPECIFIC START
      {%- if overrides.enabled %}
      overrides:
        cinder_volume_ds:
          labels:
        {%- for label, override in node_specific.items() %}
          {%- set cinder_override = override.get("cinder", {}) %}
          {%- if cinder_override %}
            {{ label }}:
          {%- endif %}
             values:
               conf:
                 standalone_backends:
                   daemonset:
                     conf:
                       {%- set overriden_enabled_backends = [] %}
                       {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
                       {%- do overriden_enabled_backends.append(backend_name) %}
                       {{ backend_name }}:
                         {%- if "lvm" in backend_opts %}
                         volumes_dir: /var/lib/cinder/volumes
                         volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
                         volume_backend_name: {{ backend_name }}
                     {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                       {%- if glance_cinder_volume_type == 'lvm' and glance_cinder_backend_name == backend_name %}
                         image_upload_use_cinder_backend: True
                         image_upload_use_internal_tenant: True
                       {%- endif %}
                     {%- endfor %}
                         {%- for key,val in backend_opts.lvm.items() %}
                         {{ key }}: {{ val }}
                         {%- endfor %}
                         {%- endif %}
                       DEFAULT:
                         enabled_backends: {{ ','.join(overriden_enabled_backends) }}
                       {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                         {%- if glance_cinder_backend_name in overriden_enabled_backends %}
                         allowed_direct_url_schemes: cinder
                         {%- endif %}
                       {%- endfor %}
                       {%- endfor %}
        {%- endfor %}
      {%- endif %}
      # NODE SPECIFIC END
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_admin_identity.yaml' %}
{% include 'base/_cache.yaml' %}
        oslo_db:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            cinder:
              username: {{ credentials.database.user.username }}
              password: {{ credentials.database.user.password }}
{%- if 'block-storage' in components_with_dedicated_messaging %}
{% include 'base/_messaging_dedicated.yaml' %}
{%- else %}
{% include 'base/_messaging_shared.yaml' %}
{%- endif %}
{% include 'base/_notifications.yaml' %}
        volume:
          enabled: false
        volumev2:
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
        volumev3:
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
      jobs:
{% include 'base/_ks_jobs.yaml' %}
{%- if cinder_db_cleanup.enabled %}
        db_purge:
          enabled: {{ cinder_db_cleanup.enabled }}
          cron: {{ cinder_db_cleanup.get("schedule", "1 0 * * 1") }}
          script:
            config:
              age: {{ cinder_db_cleanup.get("age", 30) }}
{%- endif %}
{%- endif %}
