#apiVersion: lcm.mirantis.com/v1alpha1
#kind: HelmBundle

{%- from 'macros/messaging_service_creds.j2' import messaging_service_creds %}
{%- set service = 'cinder' %}
{%- set components_with_dedicated_messaging = spec.get('features', {}).get('messaging', {}).get('components_with_dedicated_messaging', []) %}
{%- set stacklight_enabled = spec.get('features', {}).get('stacklight', {}).get('enabled', False) %}
{%- set notification_topics = ['notifications'] %}
{%- do notification_topics.append('stacklight_notifications') if stacklight_enabled %}
{%- set external_notifications_enabled = spec.get('features', {}).get('messaging', {}).get('notifications', {}).get('external', {}).get('enabled', False) %}
{%- if external_notifications_enabled %}
  {%- for topic in spec.get('features', {}).get('messaging', {}).get('notifications', {}).get('external', {}).get('topics', []) %}
    {%- do notification_topics.append(topic) %}
  {%- endfor %}
{%- endif  %}
{%- set node_specific = {} %}
{%- set overrides = namespace(enabled=false) %}
{%- for label, node_features in spec.get("nodes", {}).items() %}
  {%- if "cinder" in node_features.get("features", {}).keys() %}
    {% set overrides.enabled = true %}
  {%- endif  %}
  {%- if node_features.get("features", {}) %}
    {%- do node_specific.update({label: node_features.features}) %}
  {%- endif %}
{%- endfor %}
{%- set glance_cinder_backends = spec.get('features', {}).get('glance', {}).get('backends', {}).get('cinder', {}) %}
{%- set glance_cinder_multi = {} %}
{%- set cinder_db_cleanup = spec.get('features', {}).get('database', {}).get('cleanup', {}).get('cinder', {'enabled': true}) %}
{%- from 'macros/etcd3.j2' import get_etcd3_endpoint %}
{%- set cadf_audit = spec.get('features', {}).get('logging', {}).get('cadf', {'enabled': false}) %}
{%- set cadf_audit_driver = spec.get('features', {}).get('logging', {}).get('cadf', {}).get('driver', 'messagingv2') %}
{%- set is_backup_enabled = spec.get('features', {}).get('cinder', {}).get('backup', {}).get('enabled', True) %}
{%- if is_backup_enabled %}
{%- set backup_drivers = [] %}
{%-   for driver_name, driver in spec.get('features', {}).get('cinder', {}).get('backup', {}).get('drivers', {}).items() if driver.get('enabled', False) %}
{%-     do backup_drivers.append(driver) %}
{%-   endfor %}
{%- endif %}

spec:
  releases:
{%- if 'block-storage' in components_with_dedicated_messaging %}
  - name: openstack-cinder-rabbitmq
    chart: {{spec.common.infra.repo}}/rabbitmq
    values:
{% include 'base/_rabbitmq_images.yaml' %}
  {%- if stacklight_enabled %}
      monitoring:
        prometheus:
          enabled: true
  {%- endif %}
      pod:
        replicas:
          server: 1
      manifests:
        network_policy: false
        job_users_create: true
      volume:
        enabled: false
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_messaging_dedicated.yaml' %}
      conf:
        users:
          {{ messaging_service_creds(credentials, "cinder", ["/cinder"], enable_notifications=False)  }}
        aux_conf:
          policies:
          - vhost: cinder
            name: default-policy
            pattern: '^(?!amq\.).*'
            definition:
              message-ttl: 120000
              expires: 600000
          - vhost: cinder
            name: results_expire
            pattern: '^results\.'
            definition:
              expires: 600000
            priority: 1
          - vhost: cinder
            name: tasks_expire
            pattern: '^tasks\.'
            definition:
              expires: 600000
            priority: 1
  {%- if stacklight_enabled %}
        prometheus_exporter:
          rabbit_exporters: "overview,exchange,node"
  {%- endif %}
{%- endif %}
{%- if overrides.enabled %}
  - name: openstack-iscsi
    chart: {{spec.common.infra.repo}}/iscsi
    values:
      images:
        tags:
{%- for image in [
    "dep_check",
    "iscsi_iscsi",
    "iscsi_tgt",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      manifests:
        daemonset_tgt: true
        daemonset_iscsi: false
{%- endif %}
{%- if spec.get('migration', {}).get('cinder', {}).get('deploy_main_service', True) %}
  - name: openstack-cinder
    chart: {{spec.common.openstack.repo}}/cinder
    values:
      images:
        tags:
{%- for image in [
    "db_drop",
    "image_repo_sync",
    "cinder_api",
    "cinder_scheduler",
    "db_init",
    "dep_check",
    "cinder_db_sync",
    "cinder_db_sync_online",
    "cinder_db_purge",
    "cinder_backup",
    "ks_user",
    "ks_service",
    "cinder_volume_usage_audit",
    "cinder_backup_storage_init",
    "ks_endpoints",
    "bootstrap",
    "cinder_storage_init",
    "rabbit_init",
    "cinder_service_cleaner",
    "cinder_volume",
    "cinder_volume_daemonset",
    "cinder_drop_default_volume_type",
    "cinder_create_internal_tenant",
    "test",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      dependencies:
        static:
          db_init:
            jobs:
              - openstack-mariadb-cluster-wait
{%- if not is_ceph_enabled %}
          bootstrap:
            pod:
              - requireSameNode: false
                labels:
                  application: cinder
                  component: volume_daemonset
          drop_default_volume_type:
            pod:
              - requireSameNode: false
                labels:
                  application: cinder
                  component: volume_daemonset
{%- endif %}
      pod:
        replicas:
          api: 1
          registry: 1
        affinity:
          anti:
            type:
              backup: "requiredDuringSchedulingIgnoredDuringExecution"
{%- if overrides.enabled %}
        # migration process from source RBD based volume (hosted by sts) to
        # destination ISCSI/LVM volume requires host networking to properly
        # make an iscsi discover and login
        useHostNetwork:
          volume: true
{%- endif %}
{%- if is_ceph_enabled %}
      storage: ceph
      ceph_client:
        configmap: rook-ceph-config
        user_secret_name: {{ ceph.cinder.secrets }}
{%- endif %}
      conf:
        policy.d:
          01-controller-default.yaml: {{ service_policy }}
          02-custom.yaml: {{ spec.get("features", {}).get("policies", {}).get("cinder", {}) }}
        {%- if overrides.enabled %}
        standalone_backends:
          daemonset:
            conf:
              DEFAULT:
                cluster: ""
        {%- endif %}
        backends:
{%- set enabled_backends=[] %}
{%- if is_ceph_enabled %}
  {%- for backend, backend_config in ceph.cinder.pools.items() %}
    {%- if backend_config.role == 'volumes' %}
      {%- do enabled_backends.append(backend) %}
          {{ backend }}:
      {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
        {%- if g_backend_opts.get('backend_name') %}
          {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
        {%- endif %}
        {%- if glance_cinder_volume_type == 'rbd' and glance_cinder_backend_name == backend %}
            image_upload_use_cinder_backend: True
            image_upload_use_internal_tenant: True
        {%- endif %}
      {%- endfor %}
            volume_driver: cinder.volume.drivers.rbd.RBDDriver
            volume_backend_name: {{ backend }}
            rbd_pool: {{ backend_config.name }}
            rbd_user: {{ ceph.cinder.username }}
            rbd_ceph_conf: "/etc/ceph/ceph.conf"
    {%- endif %}
  {%- endfor %}
        ceph:
          pools:
            backup:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
            cinder.volumes:
              replication: 1
              crush_rule: replicated_ruleset
              chunk_size: 8
          config:
            global:
              mon_host: {{ ceph.mon_host }}
{%- endif %}
        cinder:
          keystone_authtoken:
            memcache_security_strategy: ENCRYPT
            memcache_secret_key: {{ credentials[0].memcached }}
            {%- if spec.openstack_version not in ("queens", "rocky", "stein") %}
            # TODO(pas-ha) register block-storage service type and catalog entry as well
            service_type: volumev3
            {%- endif %}
          DEFAULT:
            # use unique host for both cluster and non-cluster mode,
            # otherwise messaging between API and cinder-volume will
            # be broken
            host: "<None>"
# NOTE(vsaienko): active/active mode is supported by rbd starting from Rocky
# but in rocky it is not stable, so apply only from stein
{%- if is_ceph_enabled %}
  {%- if spec.openstack_version not in ['queens', 'rocky'] %}
            cluster: "cinder-ceph-cluster"
  {%- endif %}
{%- endif %}
{%- if is_backup_enabled %}
  {%- if not backup_drivers %}
            backup_driver: cinder.backup.drivers.ceph.CephBackupDriver
    {%- for backend, backend_config in ceph.cinder.pools.items() %}
      {%- if backend_config.role == 'backup' %}
            backup_ceph_user: {{ ceph.cinder.username }}
            backup_ceph_pool: {{ backend_config.name }}
      {%- endif %}
    {%- endfor %}
  {%- elif backup_drivers[0]["type"] == "s3" %}
            backup_driver: cinder.backup.drivers.s3.S3BackupDriver
    {%- for param, param_val in backup_drivers[0].items() %}
      {%- if param not in ["type", "enabled"] %}
            backup_s3_{{ param }}: {{ param_val }}
      {%- endif %}
    {%- endfor %}
  {%- endif %}
{%- endif %}
            control_exchange: cinder
{%- if is_ceph_enabled %}
            enabled_backends: {{ enabled_backends|join(',') }}
            default_volume_type: {{ enabled_backends[0] }}
{%- endif %}
            scheduler_default_filters: AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter,InstanceLocalityFilter
{%- if cadf_audit.enabled %}
          audit_middleware_notifications:
            driver: {{ cadf_audit_driver }}
{%- else %}
          audit_middleware_notifications:
            driver: noop
{%- endif %}
          oslo_messaging_notifications:
            topics: {{ notification_topics|join(',') }}
          oslo_middleware:
            max_request_body_size: 114688
          coordination:
            #There are some problems with etcd3 driver so we shouldn't use it
            #BUG: https://mirantis.jira.com/browse/PRODX-21783
            backend_url: {{ get_etcd3_endpoint(spec.openstack_version, 'etcd3gw') }}
          service_user:
            send_service_user_token: true
          backend_defaults:
            # NOTE(vsaienko): Use dedicated pool for cinder only, this will allow to improve
            # scale characteristics
            rbd_exclusive_cinder_pool: true
          # Needed for InstanceLocalityFilter
          nova:
            auth_section: keystone_authtoken
            auth_type: password
{%- if spec.get('features', {}).get('glance', {}).get("signature", {}).get("enabled", False) %}
          glance:
            verify_glance_signatures: true
{%- endif %}
        logging:
          logger_cinder:
            level: {{ spec.get('features', {}).get('logging', {}).get('cinder', {}).get('level', 'INFO') }}
          logger_os.brick:
            level: {{ spec.get('features', {}).get('logging', {}).get('cinder', {}).get('level', 'INFO') }}
      # NOTE(vsaienko): do not create backends from .Values.conf.cinder.DEFAULT.backends
      # as we have default backend in chart values rbd1 which is not used.
      # Do not create volume type for backup, as user can't use this backend directly.
      bootstrap:
        bootstrap_conf_backends: false
        volume_types:
    {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
      {%- if g_backend_opts.get('backend_name') %}
        {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
          {{ glance_cinder_backend_name }}_multiattach:
            volume_backend_name: {{ glance_cinder_backend_name }}
            multiattach: "\"<is> True\""
      {%- endif %}
    {%- endfor %}
{%- for backend in enabled_backends %}
          {{ backend }}:
            volume_backend_name: {{ backend }}
          {{ backend }}_multiattach:
            volume_backend_name: {{ backend }}
            multiattach: "\"<is> True\""
{%- endfor %}
{%- for label, override in node_specific.items() %}
  {%- set cinder_override = override.get("cinder", {}) %}
    {%- if cinder_override %}
      {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
          {{ backend_name }}:
            volume_backend_name: {{ backend_name }}
        {%- if "lvm" in backend_opts %}
          {{ backend_name }}_multiattach:
            volume_backend_name: {{ backend_name }}
            multiattach: "\"<is> True\""
        {%- endif %}
      {%- endfor %}
    {%- endif %}
{%- endfor %}
{%- if is_ceph_enabled %}
      secrets:
        rbd:
          volume: {{ ceph.cinder.secrets }}
  {%- if is_backup_enabled and not backup_drivers %}
          backup: {{ ceph.cinder.secrets }}
  {%- endif %}
{%- endif %}
      manifests:
        cron_job_db_purge: true
        job_drop_default_volume_type: true
        network_policy: false
        job_rabbit_init: false
        job_storage_init: false
        job_backup_storage_init: false
        secret_ca_bundle: true
        cron_service_cleaner: true
        cron_volume_usage_audit: false
        statefulset_backup: {{ is_backup_enabled }}
        statefulset_volume: {{ spec.features.get("cinder", {}).get("volume", {}).get("enabled", True) }}
        ceph_conf: {{ is_ceph_enabled }}
        job_clean: false
      network:
        api:
          ingress:
            annotations:
              nginx.ingress.kubernetes.io/proxy-body-size: "114688"
      # NODE SPECIFIC START
      {%- if overrides.enabled %}
      overrides:
        cinder_volume_ds:
          labels:
        {%- for label, override in node_specific.items() %}
          {%- set cinder_override = override.get("cinder", {}) %}
          {%- if cinder_override %}
            {{ label }}:
          {%- endif %}
             values:
               conf:
                 standalone_backends:
                   daemonset:
                     conf:
                       {%- set overriden_enabled_backends = [] %}
                       {%- for backend_name,backend_opts in cinder_override.get("volume", {}).get("backends", {}).items() %}
                       {%- do overriden_enabled_backends.append(backend_name) %}
                       {{ backend_name }}:
                         {%- if "lvm" in backend_opts %}
                         volumes_dir: /var/lib/cinder/volumes
                         volume_driver: cinder.volume.drivers.lvm.LVMVolumeDriver
                         volume_backend_name: {{ backend_name }}
                     {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                       {%- if glance_cinder_volume_type == 'lvm' and glance_cinder_backend_name == backend_name %}
                         image_upload_use_cinder_backend: True
                         image_upload_use_internal_tenant: True
                       {%- endif %}
                     {%- endfor %}
                         {%- for key,val in backend_opts.lvm.items() %}
                         {{ key }}: {{ val }}
                         {%- endfor %}
                         {%- endif %}
                       DEFAULT:
                         enabled_backends: {{ ','.join(overriden_enabled_backends) }}
                       {%- for g_backend_name, g_backend_opts in glance_cinder_backends.items() %}
                       {%- if g_backend_opts.get('backend_name') %}
                         {%- set glance_cinder_volume_type, glance_cinder_backend_name =  g_backend_opts['backend_name'].split(':') %}
                       {%- endif %}
                         {%- if glance_cinder_backend_name in overriden_enabled_backends %}
                         allowed_direct_url_schemes: cinder
                         {%- endif %}
                       {%- endfor %}
                       {%- endfor %}
        {%- endfor %}
      {%- endif %}
      # NODE SPECIFIC END
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_admin_identity.yaml' %}
{% include 'base/_cache.yaml' %}
        oslo_db:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            cinder:
              username: {{ credentials[0].database.user.username }}
              password: {{ credentials[0].database.user.password }}
{%- if 'block-storage' in components_with_dedicated_messaging %}
{% include 'base/_messaging_dedicated.yaml' %}
{%- else %}
{% include 'base/_messaging_shared.yaml' %}
{%- endif %}
{% include 'base/_notifications.yaml' %}
        volume:
          enabled: false
        volumev2:
{%- if spec.openstack_version not in ['queens', 'rocky', 'stein', 'train', 'ussuri', 'victoria', 'wallaby'] %}
          state:  absent
{%- endif %}
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
        volumev3:
          host_fqdn_override:
            public:
              host: cinder.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
{%- if proxy_settings is defined %}
{%- if proxy_settings.get("proxy_ca_certificate") %}
{{ proxy_settings["proxy_ca_certificate"] | indent( width=18, first=True) }}
{%- endif %}
{%- endif %}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: cinder-api
            default: cinder
            internal: cinder-api
            public:
              host: cinder
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8776
              default: 80
              internal: 8776
              public: 443
          scheme:
            default: http
            public: https
      jobs:
{% include 'base/_ks_jobs.yaml' %}
        db_purge:
          enabled: {{ cinder_db_cleanup.enabled }}
          cron: {{ cinder_db_cleanup.get("schedule", "1 0 * * 1") }}
          script:
            config:
              age: {{ cinder_db_cleanup.get("age", 30) }}
{%- endif %}
