#apiVersion: lcm.mirantis.com/v1alpha1
#kind: HelmBundle

{%- set stacklight_enabled = spec.get('features', {}).get('stacklight', {}).get('enabled', False) %}

spec:
  releases:
  - name: openstack-mariadb
    chart: {{spec.common.infra.repo}}/mariadb
    values:
      conf:
        database_conf:
          mysqld:
            wsrep_provider_options:
              # Controls parallel applying of slave actions.
              # When enabled allows full range of parallelization as determined by certification algorithm.
              # When disabled limits parallel applying window to not exceed that seen on master.
              # In other words, the action starts applying no sooner than all actions it has seen on the master are committed.
              # The default value is YES, and it causes the issue described - here https://jira.mariadb.org/browse/MDEV-22766
              # Once the issue is fixed, this can be removed. Option can be set only starting galera 3.25.
              cert.optimistic_pa: "NO"
      images:
        tags:
{%- for image in [
    "ingress",
    "mariadb_ingress",
    "prometheus_create_mysql_user",
    "image_repo_sync",
    "error_pages",
    "mariadb_backup",
    "prometheus_mysql_exporter",
    "prometheus_mysql_exporter_helm_tests",
    "dep_check",
    "mariadb",
    "mariadb_scripted_test",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
{%- if stacklight_enabled %}
      monitoring:
        prometheus:
          enabled: true
{%- endif %}
      manifests:
        network_policy: false
        job_cluster_wait: true
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
        oslo_db:
          namespace: null
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            sst:
              username: {{ galera_creds.sst.username }}
              password: {{ galera_creds.sst.password }}
            exporter:
              username: {{ galera_creds.exporter.username }}
              password: {{ galera_creds.exporter.password }}
            audit:
              username: {{ galera_creds.audit.username }}
              password: {{ galera_creds.audit.password }}
      pod:
        replicas:
          server: 3
{%- if spec.get('features', {}).get('database', {}).get('local_volumes', {}).get('enabled', False) %}
        # To have higher level of data redundency in case of local volumes - ensure pods are spread
        # across volumes placed on different nodes
        affinity:
          anti:
            type:
              default: requiredDuringSchedulingIgnoredDuringExecution
{%- endif %}
      volume:
{%- if spec.get('features', {}).get('database', {}).get('local_volumes', {}).get('enabled', False) %}
        class_name: lvp-fake-root
{%- else %}
        class_name: rook-ceph-block
{%- endif %}
